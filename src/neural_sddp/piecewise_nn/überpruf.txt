erste conditional features
[10  8  5  7 11  3  3]
Dies ist der stage
das ist der stage index
1
<class 'int'>
das ist das time embedding
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
cond feat nach Dense
128
[ -1.3880893   -7.2241516   -1.5955772   -5.6538615   -2.6236572
   5.4195275    8.645209    16.632738    -3.7065377   -0.5380528
  -6.864105    -4.3734236   -7.263309     8.581713    -2.2627127
 -10.951805     1.3706071    7.0071473   -2.4718094   -0.11492112
   6.138116   -15.623816     4.307314     9.585976    -1.7818613
   7.9883976   -6.1526413    5.098167     4.3146286    8.306476
  -4.47938      1.4028664    1.9926246   18.20748     11.254092
  -3.9477608   -9.127495    10.938296     1.7692866   -3.9472091
   4.9815226   -1.8590331   -2.896596    -1.9355787    0.68090343
 -10.736337     1.9230831   -2.2569304    7.8405075   -0.34946507
 -11.2165365    1.3889756    4.388414     6.0358853    1.545202
  -9.375855    -0.45201135   6.2415214    3.9525468   12.227073
   7.557827    -1.4878131    5.2451878   -2.0975206   -6.7518835
 -10.0967045   -7.3401604   -7.266289     2.1964223    4.041726
   0.5717908  -17.358149     1.3429285   -4.2139807   12.069285
  13.470959     5.667903     0.20359361   1.8462615   -0.25402182
   8.328888     1.8835089    2.724263    -7.4481654   -9.49464
 -17.062803   -12.130285    -1.9403255  -11.59826      1.0676727
  -7.544742    -1.1236218  -16.113916     1.8924737   -0.9196142
  -9.377539     9.734166     1.8671081   -1.6450682  -11.487395
  -0.55862856   9.8036      -6.4853086   13.841302     0.6423259
  -3.0637257   14.99183    -13.3036       1.6503528    8.115853
  -0.29219204  -4.1129594    6.175491     1.9624695    3.2002184
   3.1752207  -14.581012    -2.632465    13.020822    -5.83976
  -8.869735   -16.906954     1.7843921    9.07805      3.5373783
   3.2291503    6.427902    -4.366746  ]
conditional features
[ -1.3880893   -7.2241516   -1.5955772   -5.6538615   -2.6236572
   5.4195275    8.645209    16.632738    -3.7065377   -0.5380528
  -6.864105    -4.3734236   -7.263309     8.581713    -2.2627127
 -10.951805     1.3706071    7.0071473   -2.4718094   -0.11492112
   6.138116   -15.623816     4.307314     9.585976    -1.7818613
   7.9883976   -6.1526413    5.098167     4.3146286    8.306476
  -4.47938      1.4028664    1.9926246   18.20748     11.254092
  -3.9477608   -9.127495    10.938296     1.7692866   -3.9472091
   4.9815226   -1.8590331   -2.896596    -1.9355787    0.68090343
 -10.736337     1.9230831   -2.2569304    7.8405075   -0.34946507
 -11.2165365    1.3889756    4.388414     6.0358853    1.545202
  -9.375855    -0.45201135   6.2415214    3.9525468   12.227073
   7.557827    -1.4878131    5.2451878   -2.0975206   -6.7518835
 -10.0967045   -7.3401604   -7.266289     2.1964223    4.041726
   0.5717908  -17.358149     1.3429285   -4.2139807   12.069285
  13.470959     5.667903     0.20359361   1.8462615   -0.25402182
   8.328888     1.8835089    2.724263    -7.4481654   -9.49464
 -17.062803   -12.130285    -1.9403255  -11.59826      1.0676727
  -7.544742    -1.1236218  -16.113916     1.8924737   -0.9196142
  -9.377539     9.734166     1.8671081   -1.6450682  -11.487395
  -0.55862856   9.8036      -6.4853086   13.841302     0.6423259
  -3.0637257   14.99183    -13.3036       1.6503528    8.115853
  -0.29219204  -4.1129594    6.175491     1.9624695    3.2002184
   3.1752207  -14.581012    -2.632465    13.020822    -5.83976
  -8.869735   -16.906954     1.7843921    9.07805      3.5373783
   3.2291503    6.427902    -4.366746  ]
joint params
(10,)
joint params output
[ 3.4318058   7.2695127   1.3301265   0.78676224  0.40842575  5.1985183
 -9.341481    2.503942   -3.4009695  -3.767123  ]
Sizes für MLP
[128, 10]
params after reshape
(1, 5, 2)
Dies sind die parameter nach apply
[[[ 3.4318058   7.2695127 ]
  [ 1.3301265   0.78676224]
  [ 0.40842575  5.1985183 ]
  [-9.341481    2.503942  ]
  [-3.4009695  -3.767123  ]]]
FrozenDict({
    params: {
        Dense_0: {
            bias: (128,),
            kernel: (7, 128),
        },
        Embed_0: {
            embedding: (1, 128),
        },
        MLP_0: {
            Dense_0: {
                bias: (128,),
                kernel: (128, 128),
            },
            Dense_1: {
                bias: (10,),
                kernel: (128, 10),
            },
        },
    },
})
eval loss argumente
CondPiecewiseNN(
    # attributes
    num_vars = 1
    num_stages = 2
    hidden_size = 128
    num_pieces = 5
    num_layers = 1
    kernel_init = init
    bias_init = zeros
) FrozenDict({
    params: {
        Dense_0: {
            bias: Traced<ShapedArray(float32[128])>with<JVPTrace(level=2/2)> with
              primal = Traced<ShapedArray(float32[128])>with<DynamicJaxprTrace(level=0/2)>
              tangent = Traced<ShapedArray(float32[128])>with<JaxprTrace(level=1/2)> with
                pval = (ShapedArray(float32[128]), None)
                recipe = LambdaBinding(),
            kernel: Traced<ShapedArray(float32[7,128])>with<JVPTrace(level=2/2)> with
              primal = Traced<ShapedArray(float32[7,128])>with<DynamicJaxprTrace(level=0/2)>
              tangent = Traced<ShapedArray(float32[7,128])>with<JaxprTrace(level=1/2)> with
                pval = (ShapedArray(float32[7,128]), None)
                recipe = LambdaBinding(),
        },
        Embed_0: {
            embedding: Traced<ShapedArray(float32[1,128])>with<JVPTrace(level=2/2)> with
              primal = Traced<ShapedArray(float32[1,128])>with<DynamicJaxprTrace(level=0/2)>
              tangent = Traced<ShapedArray(float32[1,128])>with<JaxprTrace(level=1/2)> with
                pval = (ShapedArray(float32[1,128]), None)
                recipe = LambdaBinding(),
        },
        MLP_0: {
            Dense_0: {
                bias: Traced<ShapedArray(float32[128])>with<JVPTrace(level=2/2)> with
                  primal = Traced<ShapedArray(float32[128])>with<DynamicJaxprTrace(level=0/2)>
                  tangent = Traced<ShapedArray(float32[128])>with<JaxprTrace(level=1/2)> with
                    pval = (ShapedArray(float32[128]), None)
                    recipe = LambdaBinding(),
                kernel: Traced<ShapedArray(float32[128,128])>with<JVPTrace(level=2/2)> with
                  primal = Traced<ShapedArray(float32[128,128])>with<DynamicJaxprTrace(level=0/2)>
                  tangent = Traced<ShapedArray(float32[128,128])>with<JaxprTrace(level=1/2)> with
                    pval = (ShapedArray(float32[128,128]), None)
                    recipe = LambdaBinding(),
            },
            Dense_1: {
                bias: Traced<ShapedArray(float32[10])>with<JVPTrace(level=2/2)> with
                  primal = Traced<ShapedArray(float32[10])>with<DynamicJaxprTrace(level=0/2)>
                  tangent = Traced<ShapedArray(float32[10])>with<JaxprTrace(level=1/2)> with
                    pval = (ShapedArray(float32[10]), None)
                    recipe = LambdaBinding(),
                kernel: Traced<ShapedArray(float32[128,10])>with<JVPTrace(level=2/2)> with
                  primal = Traced<ShapedArray(float32[128,10])>with<DynamicJaxprTrace(level=0/2)>
                  tangent = Traced<ShapedArray(float32[128,10])>with<JaxprTrace(level=1/2)> with
                    pval = (ShapedArray(float32[128,10]), None)
                    recipe = LambdaBinding(),
            },
        },
    },
}) Traced<ShapedArray(int32[7])>with<DynamicJaxprTrace(level=0/2)> Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace(level=0/2)> Traced<ShapedArray(int32[1,5,2])>with<DynamicJaxprTrace(level=0/2)>
erste conditional features
Traced<ShapedArray(int32[7])>with<DynamicJaxprTrace(level=0/2)>
Dies ist der stage
das ist der stage index
1
<class 'int'>
das ist das time embedding
Traced<ShapedArray(int32[128], weak_type=True)>with<DynamicJaxprTrace(level=0/2)>
cond feat nach Dense
128
Traced<ShapedArray(float32[128])>with<JVPTrace(level=2/2)> with
  primal = Traced<ShapedArray(float32[128])>with<DynamicJaxprTrace(level=0/2)>
  tangent = Traced<ShapedArray(float32[128])>with<JaxprTrace(level=1/2)> with
    pval = (ShapedArray(float32[128]), None)
    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x128792830>, in_tracers=(Traced<ShapedArray(float32[128]):JaxprTrace(level=1/2)>, Traced<ShapedArray(float32[128]):JaxprTrace(level=1/2)>), out_tracer_refs=[<weakref at 0x128a16270; to 'JaxprTracer' at 0x128a16220>], out_avals=[ShapedArray(float32[128])], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(fn)', 'donated_invars': (False, False), 'inline': True, 'keep_unused': False, 'call_jaxpr': { lambda ; a:f32[128] b:f32[128]. let c:f32[128] = add a b in (c,) }}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x128a27270>, name_stack=NameStack(stack=())))
conditional features
Traced<ShapedArray(float32[128])>with<JVPTrace(level=2/2)> with
  primal = Traced<ShapedArray(float32[128])>with<DynamicJaxprTrace(level=0/2)>
  tangent = Traced<ShapedArray(float32[128])>with<JaxprTrace(level=1/2)> with
    pval = (ShapedArray(float32[128]), None)
    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x128792730>, in_tracers=(Traced<ShapedArray(float32[128]):JaxprTrace(level=1/2)>, Traced<ShapedArray(float32[128]):JaxprTrace(level=1/2)>), out_tracer_refs=[<weakref at 0x128a16d60; to 'JaxprTracer' at 0x128a16b80>], out_avals=[ShapedArray(float32[128])], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(fn)', 'donated_invars': (False, False), 'inline': True, 'keep_unused': False, 'call_jaxpr': { lambda ; a:f32[128] b:f32[128]. let c:f32[128] = mul b a in (c,) }}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x128a273b0>, name_stack=NameStack(stack=())))
joint params
(10,)
joint params output
Traced<ShapedArray(float32[10])>with<JVPTrace(level=2/2)> with
  primal = Traced<ShapedArray(float32[10])>with<DynamicJaxprTrace(level=0/2)>
  tangent = Traced<ShapedArray(float32[10])>with<JaxprTrace(level=1/2)> with
    pval = (ShapedArray(float32[10]), None)
    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x1287929f0>, in_tracers=(Traced<ShapedArray(float32[10]):JaxprTrace(level=1/2)>, Traced<ShapedArray(float32[10]):JaxprTrace(level=1/2)>), out_tracer_refs=[<weakref at 0x128a4f130; to 'JaxprTracer' at 0x128a4f0e0>], out_avals=[ShapedArray(float32[10])], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(fn)', 'donated_invars': (False, False), 'inline': True, 'keep_unused': False, 'call_jaxpr': { lambda ; a:f32[10] b:f32[10]. let c:f32[10] = add a b in (c,) }}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x128a330f0>, name_stack=NameStack(stack=())))
Sizes für MLP
[128, 10]
params after reshape
(1, 5, 2)
Dies sind die parameter nach apply
Traced<ShapedArray(float32[1,5,2])>with<JVPTrace(level=2/2)> with
  primal = Traced<ShapedArray(float32[1,5,2])>with<DynamicJaxprTrace(level=0/2)>
  tangent = Traced<ShapedArray(float32[1,5,2])>with<JaxprTrace(level=1/2)> with
    pval = (ShapedArray(float32[1,5,2]), None)
    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x128792950>, in_tracers=(Traced<ShapedArray(float32[10]):JaxprTrace(level=1/2)>,), out_tracer_refs=[<weakref at 0x128a46e00; to 'JaxprTracer' at 0x128a46a90>], out_avals=[ShapedArray(float32[1,5,2])], primitive=reshape, params={'new_sizes': (1, 5, 2), 'dimensions': None}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x1289ffeb0>, name_stack=NameStack(stack=())))
pred_params
(1, 5, 2)
Traced<ShapedArray(float32[1,5,2])>with<JVPTrace(level=2/2)> with
  primal = Traced<ShapedArray(float32[1,5,2])>with<DynamicJaxprTrace(level=0/2)>
  tangent = Traced<ShapedArray(float32[1,5,2])>with<JaxprTrace(level=1/2)> with
    pval = (ShapedArray(float32[1,5,2]), None)
    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x128792950>, in_tracers=(Traced<ShapedArray(float32[10]):JaxprTrace(level=1/2)>,), out_tracer_refs=[<weakref at 0x128a46e00; to 'JaxprTracer' at 0x128a46a90>], out_avals=[ShapedArray(float32[1,5,2])], primitive=reshape, params={'new_sizes': (1, 5, 2), 'dimensions': None}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x1289ffeb0>, name_stack=NameStack(stack=())))
target
(1, 5, 2)
LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL
(1, 5, 2)
(1, 5, 2)
LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL
points_x
Traced<ShapedArray(int32[5,2])>with<BatchTrace(level=3/4)> with
  val = Traced<ShapedArray(int32[1,5,2])>with<DynamicJaxprTrace(level=0/4)>
  batch_dim = 0
Traced<ShapedArray(float32[5,2])>with<BatchTrace(level=3/4)> with
  val = Traced<ShapedArray(float32[1,5,2])>with<JVPTrace(level=2/4)> with
    primal = Traced<ShapedArray(float32[1,5,2])>with<DynamicJaxprTrace(level=0/4)>
    tangent = Traced<ShapedArray(float32[1,5,2])>with<JaxprTrace(level=1/4)> with
      pval = (ShapedArray(float32[1,5,2]), None)
      recipe = LambdaBinding()
  batch_dim = 0
points_y
points_x
Traced<ShapedArray(int32[5,2])>with<BatchTrace(level=3/5)> with
  val = Traced<ShapedArray(int32[1,5,2])>with<DynamicJaxprTrace(level=0/5)>
  batch_dim = 0
Traced<ShapedArray(float32[5,2])>with<BatchTrace(level=3/5)> with
  val = Traced<ShapedArray(float32[1,5,2])>with<JVPTrace(level=2/5)> with
    primal = Traced<ShapedArray(float32[1,5,2])>with<DynamicJaxprTrace(level=0/5)>
    tangent = Traced<ShapedArray(float32[1,5,2])>with<JaxprTrace(level=1/5)> with
      pval = (ShapedArray(float32[1,5,2]), None)
      recipe = LambdaBinding()
  batch_dim = 0
points_y
model loss 244.79866
apply von modell
erste conditional features
[10  8  5  7 11  3  3]
Dies ist der stage
das ist der stage index
1
<class 'int'>
das ist das time embedding
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
cond feat nach Dense
128
[ -2.3480837   -6.2641582   -0.63558376  -4.693867    -1.6636637
   6.379521     9.605203    15.672746    -2.7465441   -1.4980457
  -5.904112    -5.3334174   -6.303315     9.541706    -3.2227063
 -11.911798     2.3305998    6.0471535   -1.5118161   -1.0749145
   5.178123   -16.583809     5.267308     8.625984    -0.8218675
   7.028405    -7.1126347    4.138173     3.3546345    7.3464823
  -3.5193858    2.3628597    2.9526181   17.247484    12.214086
  -2.987767    -8.167501    11.898289     0.80929303  -2.987216
   4.021529    -0.8990395   -1.9366026   -0.97558516  -0.2790898
  -9.776343     2.8830767   -3.2169242    6.8805137    0.6105283
 -10.256542     2.348969     3.4284203    6.995879     2.5051956
 -10.335849    -1.4120046    7.2015142    4.9125404   13.187065
   8.51782     -0.52781993   6.205182    -1.137527    -7.711877
  -9.136711    -8.300153    -8.226282     1.2364286    3.0817325
  -0.38820288 -16.398155     0.38293478  -3.2539873   11.109291
  12.510966     6.6278963    1.1635871    2.8062549   -1.2140154
   9.288882     2.8435023    3.684257    -6.4881716  -10.454633
 -16.10281    -11.170292    -2.9003196  -10.638267     0.1076791
  -6.5847487   -0.16362867 -17.073912     2.8524673   -1.8796073
 -10.337532     8.774173     2.8271017   -0.6850745  -10.527403
   0.4013652   10.763593    -7.445303    12.88131      1.6023192
  -4.023719    14.031836   -12.343608     0.69035923   7.1558595
  -1.2521857   -3.1529665    5.215497     2.9224627    4.1602116
   2.2152271  -13.621018    -1.6724721   13.980815    -4.879766
  -7.9097414  -15.94696      0.8243988    8.118056     2.5773852
   4.1891446    7.387895    -5.3267403 ]
conditional features
[ -2.3480837   -6.2641582   -0.63558376  -4.693867    -1.6636637
   6.379521     9.605203    15.672746    -2.7465441   -1.4980457
  -5.904112    -5.3334174   -6.303315     9.541706    -3.2227063
 -11.911798     2.3305998    6.0471535   -1.5118161   -1.0749145
   5.178123   -16.583809     5.267308     8.625984    -0.8218675
   7.028405    -7.1126347    4.138173     3.3546345    7.3464823
  -3.5193858    2.3628597    2.9526181   17.247484    12.214086
  -2.987767    -8.167501    11.898289     0.80929303  -2.987216
   4.021529    -0.8990395   -1.9366026   -0.97558516  -0.2790898
  -9.776343     2.8830767   -3.2169242    6.8805137    0.6105283
 -10.256542     2.348969     3.4284203    6.995879     2.5051956
 -10.335849    -1.4120046    7.2015142    4.9125404   13.187065
   8.51782     -0.52781993   6.205182    -1.137527    -7.711877
  -9.136711    -8.300153    -8.226282     1.2364286    3.0817325
  -0.38820288 -16.398155     0.38293478  -3.2539873   11.109291
  12.510966     6.6278963    1.1635871    2.8062549   -1.2140154
   9.288882     2.8435023    3.684257    -6.4881716  -10.454633
 -16.10281    -11.170292    -2.9003196  -10.638267     0.1076791
  -6.5847487   -0.16362867 -17.073912     2.8524673   -1.8796073
 -10.337532     8.774173     2.8271017   -0.6850745  -10.527403
   0.4013652   10.763593    -7.445303    12.88131      1.6023192
  -4.023719    14.031836   -12.343608     0.69035923   7.1558595
  -1.2521857   -3.1529665    5.215497     2.9224627    4.1602116
   2.2152271  -13.621018    -1.6724721   13.980815    -4.879766
  -7.9097414  -15.94696      0.8243988    8.118056     2.5773852
   4.1891446    7.387895    -5.3267403 ]
joint params
(10,)
joint params output
[-31.12454  -14.253132  34.10424   12.606367  22.844995 -11.710573
  47.04159   -7.120484  27.736809   6.99356 ]
Sizes für MLP
[128, 10]
params after reshape
(1, 5, 2)
Dies sind die parameter nach apply
[[[-31.12454  -14.253132]
  [ 34.10424   12.606367]
  [ 22.844995 -11.710573]
  [ 47.04159   -7.120484]
  [ 27.736809   6.99356 ]]]
dies ist der outpur
[[[-31.12454  -14.253132]
  [ 34.10424   12.606367]
  [ 22.844995 -11.710573]
  [ 47.04159   -7.120484]
  [ 27.736809   6.99356 ]]]
ende von apply von modell
