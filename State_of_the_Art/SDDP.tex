\subsection{Stochastic Dual Dynamic Programming}\label{Stochastic dual dynamic programming}
Stochastic dual dynamic programming (SDDP), first presented by Pereira and Pinto \cite{PereiraPinto1991}, has become one of the standard methods to solve multistage stochastic optimization problems.
In its essence, SDDP is a nested Benders decomposition with sampling, which solves the problem with the exponentially increasing number of necessary constraints \cite{Fuellner_SDDP_TUT}. \\
Due to the sampling, the SDDP approach can overcome this downside by not calculating all constraints that describe the value function for every possible scenario, but using a subset that is obtained by sampling scenarios.
The efficiency improvement occurs if not all constrains have to be calculated to ensure feasibility and optimality.
That way, the exponential increase in necessary additional constraints is not as severe.

One drawback of the SDDP approach is that according to \cite{NSDDP} the number of generated cutting planes to approximate the value function $V_{t+1}$ increases exponentially in the dimension of the decision variable $x_t$. This severely limits the size of problem statements, that can be practically solved \cite{NSDDP}.
One approach that overcomes this downside will be presented in section \ref{Neural stochastic dual dynamic programming}.

The SDDP algorithm has three major steps; sampling the scenario-tree, the forward pass and the backward pass.
The parameters of the SDDP are the number of samples to be drawn from the scenario-tree and an initial approximation of the value function, which is sometimes set to be zero or infinity, depending on whether the problem is a maximization or minimization problem \cite{Powell_Perspectives_of_ADP}. \\
One sample represents one scenario in the scenario-tree, so a path from the root to one leaf.
For each of these sample paths the forward and backward pass is executed.
The forward pass traverses all $T$ stages of the scenario-tree along the sampled path and solves each occurring two-stage problem with the current value function approximation.
Since each stage from $t=1$ to $T-1$ has its own value function approximation, these approximations get updated during the backward pass.
In the backward pass, starting from the last stage $t=T$, each value function approximation $\Bar{V}_{t}$
is updated with the respective optimal decisions $x_{t-1}$ from the forward pass \cite{NSDDP}.

This procedure subsequently adds optimality cuts like in the Benders decomposition.
For each stage an optimal solution with respect to the current value function approximation is calculated.
Since this approach iteratively adds cuts to the main problem to obtain the value of the expected value function, these approximations provide lower bounds to the real expected value function.

According to \cite{PereiraPinto1991}, an upper bound for the expected value can be generated by calculating the objective value of a feasible point.
This provides a measure of how good the current approximation is and also provides a termination criterion for the algorithm.
If the difference between the calculated lower and upper bound is smaller than some threshold, the algorithm terminates.

According to \cite{SDDP_Solver_Paper}, due to these sampling techniques and the exploitation of the convexity of the value function, the SDDP provides linear complexity in the number of stages and hence solves the shortcomings of the nested Benders decomposition and L-shaped method. 
However, according to \cite{NSDDP}, SDDP still can require an exponential number of iterations with respect to the number of decision variables.
The approach of Neural stochastic dual dynamic programming is an approach that tries to overcome these limitations.
