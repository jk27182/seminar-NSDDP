@incollection{PowerFlowGrossmannBuch,
title = {Multi-objective optimization of energy networks under demand uncertainty},
editor = {Zdravko Kravanja and Miloš Bogataj},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {38},
pages = {2319-2324},
year = {2016},
booktitle = {26th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63428-3.50391-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044463428350391X},
author = {Edwin Zondervan and Ignacio E. Grossmann},
keywords = {add three to five keywords here},
abstract = {In this work we extend a deterministic security constrained unit commitment model in two directions. First we assess multi-objectivity; where we balance the generator costs with the carbon dioxide emmissions. Second we extend the model to a first-stage stochastic model that incorporates different realizations of the demand.}
}



@article{OptimalPowerFlow,
author = {Stephen Frank and Steffen Rebennack},
title = {An introduction to optimal power flow: Theory, formulation, and examples},
journal = {IIE Transactions},
volume = {48},
number = {12},
pages = {1172-1197},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/0740817X.2016.1189626},

URL = { 
        https://doi.org/10.1080/0740817X.2016.1189626
    
},
eprint = { 
        https://doi.org/10.1080/0740817X.2016.1189626
    
}

}




@book{ComputerVisionBook,
  author    = {Richard Szeliski},
  title     = {Computer Vision - Algorithms and Applications, Second Edition},
  series    = {Texts in Computer Science},
  publisher = {Springer},
  year      = {2022}
}
@misc{ggo2,
    author = "Stein, Oliver",
    title = "Gemischt-ganzzahlige {O}ptimierung 2",
    year = "2022",
}
@misc{adam_optimizer,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Birge1980SolutionMF,
  title={Solution methods for stochastic dynamic linear programs},
  author={J. Birge},
  year={1980}
}
@article{LShapedVanSlyke,
author = {Van Slyke, R. M. and Wets, Roger},
title = {L-Shaped Linear Programs with Applications to Optimal Control and Stochastic Programming},
journal = {SIAM Journal on Applied Mathematics},
volume = {17},
number = {4},
pages = {638-663},
year = {1969},
doi = {10.1137/0117061},

URL = { 
        https://doi.org/10.1137/0117061
    
},
eprint = { 
        https://doi.org/10.1137/0117061
    
}
,
    abstract = { This paper gives an algorithm for L-shaped linear programs which arise naturally in optimal control problems with state constraints and stochastic linear programs (which can be represented in this form with an infinite number of linear constraints). The first section describes a cutting hyperplane algorithm which is shown to be equivalent to a partial decomposition algorithm of the dual program. The two last sections are devoted to applications of the cutting hyperplane algorithm to a linear optimal control problem and stochastic programming problems. }
}

@article{BIRGELSHAPED,
title = {A multicut algorithm for two-stage stochastic linear programs},
journal = {European Journal of Operational Research},
volume = {34},
number = {3},
pages = {384-392},
year = {1988},
issn = {0377-2217},
doi = {https://doi.org/10.1016/0377-2217(88)90159-2},
url = {https://www.sciencedirect.com/science/article/pii/0377221788901592},
author = {John R. Birge and François V. Louveaux},
keywords = {Stochastic programming, outer linearization, cutting plane methods},
abstract = {Outer linearization methods, such as Van Slyke and West's L-shaped method for stochastic linear programs, generally apply a single cut on the nonlinear objective at each major iteration. The structure of stochastic programs allows for several cuts to be placed at once. This paper describes a multicut algorithm to carry out this procedure. It presents experimental and theoretical justification for reductions in major iterations.}
}
@book{floudas1995,
  title={Nonlinear and mixed-integer optimization: fundamentals and applications},
  author={Floudas, Christodoulos A},
  year={1995},
  publisher={Oxford University Press}
}

@book{TwenteBuchKapitel3,
author = {Boucherie, R.J. and Dijk, Nico},
year = {2017},
pages = {},
title = {Markov Decision Processes in Practice},
volume = {248},
isbn = {978-3-319-47764-0},
doi = {10.1007/978-3-319-47766-4}
}

@article{POWELL2019795,
title = {A unified framework for stochastic optimization},
journal = {European Journal of Operational Research},
volume = {275},
number = {3},
pages = {795-821},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2018.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0377221718306192},
author = {Warren B. Powell},
keywords = {Dynamic programming, Stochastic programming, Bandit problems, Reinforcement learning, Robust optimization, Simulation optimization},
abstract = {Stochastic optimization is an umbrella term that includes over a dozen fragmented communities, using a patchwork of sometimes overlapping notational systems with algorithmic strategies that are suited to specific classes of problems. This paper reviews the canonical models of these communities, and proposes a universal modeling framework that encompasses all of these competing approaches. At the heart is an objective function that optimizes over policies that is standard in some approaches, but foreign to others. We then identify four meta-classes of policies that encompasses all of the approaches that we have identified in the research literature or industry practice. In the process, we observe that any adaptive learning algorithm, whether it is derivative-based or derivative-free, is a form of policy that can be tuned to optimize either the cumulative reward (similar to multi-armed bandit problems) or final reward (as is used in ranking and selection or stochastic search). We argue that the principles of bandit problems, long a niche community, should become a core dimension of mainstream stochastic optimization.}
}


@inproceedings{SDDP_Solver_Paper,
  title={A Python package for multi-stage stochastic programming},
  author={Lingquan Ding and Shabbir Ahmed and Alexander Shapiro},
  year={2019}
}

@article{NSDDP,
  author    = {Hanjun Dai and
               Yuan Xue and
               Zia Syed and
               Dale Schuurmans and
               Bo Dai},
  title     = {Neural Stochastic Dual Dynamic Programming},
  journal   = {CoRR},
  volume    = {abs/2112.00874},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.00874},
  eprinttype = {arXiv},
  eprint    = {2112.00874},
  timestamp = {Tue, 07 Dec 2021 12:15:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-00874.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{continous_time_stochastic_control,
title = {Continuous-time Stochastic Control and Optimization with Financial Applications},
series = {Stochastic Modelling and Applied Probability},
editor = {Huy{\^e}n Pham},
address = {Berlin, Heidelberg},
publisher = {Springer Berlin Heidelberg},
year = {2009},
isbn = {9783540895008},
size = {Online-Ressource (digital)},
keywords = {Finanzmathematik / Stochastische Optimierung ; Finance / Systems theory / Distribution (Probability theory) / Mathematics / Mathematical optimization / Probabilities. / Calculus of variations. / Economics, Mathematical . / System theory. / Game theory.},
bestand = {Universitätsbibliothek Freiburg <25>;
Saarländische Universitäts- und Landesbibliothek <291>;
Universität Mannheim, Universitätsbibliothek <180>;
Karlsruher Institut für Technologie (KIT) - KIT-Bibliothek <90>;
Technische Universität Chemnitz <Ch 1>;
TU Bergakademie Freiberg, Universitätsbibliothek <105>;
Oberwolfach, Mathematisches Forschungsinstitut Oberwolfach <Frei 3c> [Signatur: eBook Springer];
},
URL = {https://swbplus.bsz-bw.de/bsz308023803cov.jpg ; https://swbplus.bsz-bw.de/bsz307909050kap.htm ; https://zbmath.org/?q=an:1165.93039https://doi.org/10.1007/978-3-540-89500-8},
}

@inbook{example_powell_fleet_management,
author = {Warren B. Powell and Huseyin Topaloglu},
title = {12. Fleet Management},
booktitle = {Applications of Stochastic Programming},
chapter = {},
pages = {185-215},
doi = {10.1137/1.9780898718799.ch12},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718799.ch12},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898718799.ch12},
    abstract = { 12.1 Introduction The fleet management problem, in its simplest form, involves managing fleets of equipment to meet customer requests as they evolve over time. The equipment might be containers which hold freight (ocean containers, truck trailers, boxcars), equipment such as locomotives, truck tractors, taxicabs, or business jets (companies in the fractional jet ownership business may own up to 1,000 jets). The equipment has to serve customers (people or freight) who typically want to move from one location to the next. We make the assumption throughout that a piece of equipment can serve one request at a time. One of the challenges of fleet management is that customer requests arrive randomly over time, often requiring service within a narrow interval. Since it can take from several days to more than a week to move transportation equipment over long distances, it is not possible to wait until a customer request is known before moving the equipment. As a result, it is necessary to move equipment to serve demands before they are known. In actual applications, there are other sources of randomness, such as transit times and equipment failures. Fleet management problems in practice are quite rich, and it is helpful to focus on a particular application to provide a motivating context. In this chapter we use the classic problem of car distribution in railroads. The problem of optimizing the flows of empty freight cars for railroads is typically formulated as a textbook transportation problem. There are supplies of empty cars and customers placing orders for these cars. }
}

@inbook{example_supply_chain_optimization,
author = {A. Tomasgard and E. H{\o}eg},
title = {14. A Supply Chain Optimization Model for the Norwegian Meat Cooperative},
booktitle = {Applications of Stochastic Programming},
chapter = {},
pages = {253-276},
doi = {10.1137/1.9780898718799.ch14},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718799.ch14},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898718799.ch14},
    abstract = { 14.1 Introduction The food industry is changing rapidly as there is increasing focus on efficiency at all levels in the food production value chain. Producers face requirements to become more responsive to fast changes in the markets, and main customers, like the supermarket chains, require more influence on logistic processes and product development. At the same time there is pressure to reduce both cost and lead times. Typical operations in the supply chain for meat products include receiving raw materials, coordinating production and distribution, deciding on inventory levels for raw materials and finished products, and sales of intermediate and finished products. The main difference from traditional goods production is that the raw material and the products are usually fresh and have limited durability. One keyword in supply chain optimization is coordination. It is a challenge to provide customers simultaneously with higher service level and lower costs. In most industries it is important to coordinate plans between different regions and between the different levels in the supply chain to meet these goals. By enforcing better coordination and thereby capacity utilization, the companies try to avoid suboptimal decisions at every stage in the production and distribution processes. Coordination is essential because demands in different regions are not perfectly correlated. Different regions can share safety inventories and buffers, and the available resources and raw material can be utilized from a global perspective rather than with focus on local capacities and local demand. Also, coordination makes it possible to specialize production capabilities and to share production capacities for peak production between different regions and different products. }
}

@article{example_portfolio_opt,
author = {de Melo, Maisa Kely and Cardoso, Rodrigo T. N. and Jesus, Tales A.},
title = {MultiObjective Dynamic Optimization of Investment Portfolio Based on Model Predictive Control},
journal = {SIAM Journal on Control and Optimization},
volume = {60},
number = {1},
pages = {104-123},
year = {2022},
doi = {10.1137/20M1346420},

URL = { 
        https://doi.org/10.1137/20M1346420
    
},
eprint = { 
        https://doi.org/10.1137/20M1346420
    
}
,
    abstract = { In this paper, a multiobjective model predictive control (MO-MPC) for portfolio selection is proposed. The objective functions are defined using a multiperiod format through the receding horizon strategy, considering the expected wealth, the variance, and the conditional value at risk as the objective function to be optimized, including transaction costs, self-financing, and investment limits for each asset. A Pareto front is obtained in each step by a multiobjective genetic algorithm, and a Pareto optimal point is chosen as the control action applied to the system. This choice is made based on a selection criterion according to the investor profile. Finally, the performance of the MO-MPC facing extreme situations of the financial market is investigated through numerical experiments using data from the Brazilian stock exchange. Simulation results show that the MO-MPC can deal with the dynamic and unstable scenario, efficiently tracing the trade-off between risk and return, managing the transaction costs and bounds. }
}



@Article{example_power_scheduling_Nowak2000,
author={Nowak, Matthias P.
and R{\"o}misch, Werner},
title={Stochastic Lagrangian Relaxation Applied to Power Scheduling in a Hydro-Thermal System under Uncertainty},
journal={Annals of Operations Research},
year={2000},
month={Dec},
day={01},
volume={100},
number={1},
pages={251-272},
abstract={A dynamic (multi-stage) stochastic programming model for the weekly cost-optimal generation of electric power in a hydro-thermal generation system under uncertain demand (or load) is developed. The model involves a large number of mixed-integer (stochastic) decision variables and constraints linking time periods and operating power units. A stochastic Lagrangian relaxation scheme is designed by assigning (stochastic) multipliers to all constraints coupling power units. It is assumed that the stochastic load process is given (or approximated) by a finite number of realizations (scenarios) in scenario tree form. Solving the dual by a bundle subgradient method leads to a successive decomposition into stochastic single (thermal or hydro) unit subproblems. The stochastic thermal and hydro subproblems are solved by a stochastic dynamic programming technique and by a specific descent algorithm, respectively. A Lagrangian heuristics that provides approximate solutions for the first stage (primal) decisions starting from the optimal (stochastic) multipliers is developed. Numerical results are presented for realistic data from a German power utility and for numbers of scenarios ranging from 5 to 100 and a time horizon of 168 hours. The sizes of the corresponding optimization problems go up to 200{\thinspace}000 binary and 350{\thinspace}000 continuous variables, and more than 500{\thinspace}000 constraints.},
issn={1572-9338},
doi={10.1023/A:1019248506301},
url={https://doi.org/10.1023/A:1019248506301}
}


@article{Fuellner_SDDP_TUT,
author={F\"ullner, Christian and Rebennack, Steffen},
title={Stochastic dual dynamic programming and its variants},
year=2021,
publisher={Preprint, Karlsruhe Institute of Technology, 2021}
}


@Inbook{Louveaux2009,
author="Louveaux, Francois
and Birge, John R.",
editor="Floudas, Christodoulos A.
and Pardalos, Panos M.",
title="Two-stage stochastic programs with recourseTwo-Stage Stochastic Programs with Recourse",
bookTitle="Encyclopedia of Optimization",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="3959--3961",
abstract="Keywords",
isbn="978-0-387-74759-0",
doi="10.1007/978-0-387-74759-0_691",
url="https://doi.org/10.1007/978-0-387-74759-0_691"
}

@book{NorvigRussell2016,
    title = {{Artificial Intelligence: A Modern Approach}},
    year = {2016},
    author = {Russell, S. J. and Norvig, P.},
    edition = {3},
    pages = {1152},
    publisher = {Pearson Education Limited},
    url = {http://thuvien.thanglong.edu.vn:8081/dspace/bitstream/DHTL_123456789/4010/1/CS503-2.pdf},
    address = {Harlow}
}

@Misc{David_Silver2015,
author = {David Silver},
title = {Lectures on 
Reinforcement Learning},howpublished = {\textsc{url:}~\url
{https://www.davidsilver.uk/teaching/}},year = {2015}}

@article{Hastings1970MonteCS,
  title={Monte Carlo Sampling Methods Using Markov Chains and Their Applications},
  author={W. Hastings},
  journal={Biometrika},
  year={1970},
  volume={57},
  pages={97-109}
}


@ARTICLE{NoFreeLunch,
  author={D. H. {Wolpert} and W. G. {Macready}},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={No free lunch theorems for optimization}, 
  year={1997},
  volume={1},
  number={1},
  pages={67-82},
  doi={10.1109/4235.585893}}
% ------------------------------------------------------------------------------------------------------------------------
%LP Referenzen
%Wurde 1947 von ihm entworfen
@article{dantzig1951maximization,
  title={Maximization of a linear function of variables subject to linear inequalities},
  author={Dantzig, George B},
  journal={Activity analysis of production and allocation},
  volume={13},
  pages={339--347},
  year={1951}
}
% G.B Dantzig: Maximization of a linear function of variables subject to linear inequalities, !!!! 1947 !!!!!. Published pp. 339–347 in T.C. Koopmans (ed.):Activity Analysis of Production and Allocation, New York-London 1951 (Wiley & Chapman-Hall)

%Einführung in das OR
@book{Einfuehrung_in_das_OR,
title = {Operations Research},
series = {Springer-Lehrbuch},
author = {Nickel, Stefan and Stein, Oliver and Waldmann, Karl-Heinz},
address = {Berlin},
publisher = {Springer Gabler},
year = {2014},
edition = {2., korr. und aktualisierte Aufl.},
isbn = {9783642543678; 3642543677},
size = {XII, 385 S. : graph. Darst. ; 240 mm x 168 mm},
keywords = {Operations Research ; Operations Research ; Operations Research / Lehrbuch},
},
% ------------------------------------------------------------------------------------------------------------------------



@book{boyd_vandenberghe_2004, place={Cambridge}, title={Convex Optimization}, DOI={10.1017/CBO9780511804441}, publisher={Cambridge University Press}, author={Boyd, Stephen and Vandenberghe, Lieven}, year={2004}}



% --------------------------------------------------------------------------------------------------------------------------
%LP Ansatz für ADP Farias und van Roy
@article{Schweizer_Seidmann_LP_Ansatz_DynOpt,
title = {Generalized polynomial approximations in Markovian decision processes},
journal = {Journal of Mathematical Analysis and Applications},
volume = {110},
number = {2},
pages = {568-582},
year = {1985},
issn = {0022-247X},
doi = {https://doi.org/10.1016/0022-247X(85)90317-8},
url = {https://www.sciencedirect.com/science/article/pii/0022247X85903178},
author = {Paul J Schweitzer and Abraham Seidmann},
abstract = {Fitting the value function in a Markovian decision process by a linear superposition of M basis functions reduces the problem dimensionality from the number of states down to M, with good accuracy retained if the value function is a smooth function of its argument, the state vector. This paper provides, for both the discounted and undiscounted cases, three algorithms for computing the coefficients in the linear superposition: linear programming, policy iteration, and least squares.}
}
@article{farias_VanRoy_ConstraintSampling,
 ISSN = {0364765X, 15265471},
 URL = {http://www.jstor.org/stable/30035661},
 abstract = {In the linear programming approach to approximate dynamic programming, one tries to solve a certain linear program-the ALP-that has a relatively small number K of variables but an intractable number M of constraints. In this paper, we study a scheme that samples and imposes a subset of m ≪ M constraints. A natural question that arises in this context is: How must m scale with respect to K and M in order to ensure that the resulting approximation is almost as good as one given by exact solution of the ALP? We show that, given an idealized sampling distribution and appropriate constraints on the K variables, m can be chosen independently of M and need grow only as a polynomial in K. We interpret this result in a context involving controlled queueing networks.},
 author = {Daniela Pucci de Farias and Benjamin Van Roy},
 journal = {Mathematics of Operations Research},
 number = {3},
 pages = {462--478},
 publisher = {INFORMS},
 title = {On Constraint Sampling in the Linear Programming Approach to Approximate Dynamic Programming},
 volume = {29},
 year = {2004}
}

@article{farias_vanRoy_LinearProgrammingApproach_to_ADP,
author = {de Farias, D. P. and Van Roy, B.},
title = {The Linear Programming Approach to Approximate Dynamic Programming},
journal = {Operations Research},
volume = {51},
number = {6},
pages = {850-865},
year = {2003},
doi = {10.1287/opre.51.6.850.24925},
URL = {https://doi.org/10.1287/opre.51.6.850.24925},
eprint = {https://doi.org/10.1287/opre.51.6.850.24925}
,
    abstract = { The curse of dimensionality gives rise to prohibitive computational requirements that render infeasible the exact solution of large-scale stochastic control problems. We study an efficient method based on linear programming for approximating solutions to such problems. The approach “fits” a linear combination of pre-selected basis functions to the dynamic programming cost-to-go function. We develop error bounds that offer performance guarantees and also guide the selection of both basis functions and “state-relevance weights” that influence quality of the approximation. Experimental results in the domain of queueing network control provide empirical support for the methodology. }
}







% ------------------------------------------------------------------------------------------------------------------------
% Markov Decision Processes 

@book{Puterman_MDP,
author = {Puterman, Martin L.},
title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
year = {1994},
isbn = {0471619779},
publisher = {John Wiley and Sons, Inc.},
address = {USA},
edition = {1st},
abstract = {From the Publisher:The past decade has seen considerable theoretical and applied research on Markov decision processes, as well as the growing use of these models in ecology, economics, communications engineering, and other fields where outcomes are uncertain and sequential decision-making processes are needed. A timely response to this increased activity, Martin L. Puterman's new work provides a uniquely up-to-date, unified, and rigorous treatment of the theoretical, computational, and applied research on Markov decision process models. It discusses all major research directions in the field, highlights many significant applications of Markov decision processes models, and explores numerous important topics that have previously been neglected or given cursory coverage in the literature. Markov Decision Processes focuses primarily on infinite horizon discrete time models and models with discrete time spaces while also examining models with arbitrary state spaces, finite horizon models, and continuous-time discrete state models. The book is organized around optimality criteria, using a common framework centered on the optimality (Bellman) equation for presenting results. The results are presented in a "theorem-proof" format and elaborated on through both discussion and examples, including results that are not available in any other book. A two-state Markov decision process model, presented in Chapter 3, is analyzed repeatedly throughout the book and demonstrates many results and algorithms. Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria. It also explores several topics that have received little or no attention in other books, including modified policy iteration, multichain models with average reward criterion, and sensitive optimality. In addition, a Bibliographic Remarks section in each chapter comments on relevant historic}
}
@book{MDP_Howard1960,
  address = {Cambridge, MA},
  author = {Howard, R. A.},
  publisher = {MIT Press},
  title = {Dynamic Programming and Markov Processes},
  year = {1960}
}

@article{WHITE19891,
title = {Markov decision processes},
journal = {European Journal of Operational Research},
volume = {39},
number = {1},
pages = {1-16},
year = {1989},
issn = {0377-2217},
doi = {https://doi.org/10.1016/0377-2217(89)90348-2},
url = {https://www.sciencedirect.com/science/article/pii/0377221789903482},
author = {Chelsea C. White and Douglas J. White},
keywords = {Markov decision programming},
abstract = {A review is given of an optimization model of discrete-stage, sequential decision making in a stochastic environment, called the Markov decision process (MDP). This review presents an overview of theoretical and computational results, applications, several generalizations of the standard MDP problem formulation, and future directions for research. The reference list contains seminal papers, key texts, and surveys for the interested reader.}
}

% Ende Markov Decision Processes
% -----------------------------------------------------------------------------------------------------------------

@article{Tsvetan_Powell_Regularized_Decomposition,
author = {Asamov, Tsvetan and Powell, Warren B.},
title = {Regularized Decomposition of High-Dimensional Multistage Stochastic Programs with Markov Uncertainty},
journal = {SIAM Journal on Optimization},
volume = {28},
number = {1},
pages = {575-595},
year = {2018},
doi = {10.1137/16M1072231},

URL = {https://doi.org/10.1137/16M1072231},
eprint = {https://doi.org/10.1137/16M1072231}

}
@misc{asamov2016sddp,
    title={SDDP vs. ADP: The Effect of Dimensionality in Multistage Stochastic Optimization for Grid Level Energy Storage},
    author={Tsvetan Asamov and Daniel F. Salas and Warren B. Powell},
    year={2016},
}



%Stochastische Optimierung

@article{vanSlyke_Wets_LShaped,
author = {Van Slyke, R. M. and Wets, Roger},
title = {L-Shaped Linear Programs with Applications to Optimal Control and Stochastic Programming},
journal = {SIAM Journal on Applied Mathematics},
volume = {17},
number = {4},
pages = {638-663},
year = {1969},
doi = {10.1137/0117061},

URL = { 
        https://doi.org/10.1137/0117061
    
},
eprint = { 
        https://doi.org/10.1137/0117061
    
}

}

%Artikel von Benders für Benders Dekomposition
@Article{Benders1962,
author={Benders, J. F.},
title={Partitioning procedures for solving mixed-variables programming problems},
journal={Numerische Mathematik},
year={1962},
volume={4},
number={1},
pages={238-252},
issn={0945-3245},
doi={10.1007/BF01386316},
url={https://doi.org/10.1007/BF01386316}
}


% Dual Dynamic Programing Paper -> VFA durch hinzufügen von Schnittebenen mit Hilfe von dualen Problem
@Article{PereiraPinto1991,
author={Pereira, M. V. F.
and Pinto, L. M. V. G.},
title={Multi-stage stochastic optimization applied to energy planning},
journal={Mathematical Programming},
year={1991},
volume={52},
number={1},
pages={359-375},
abstract={This paper presents a methodology for the solution of multistage stochastic optimization problems, based on the approximation of the expected-cost-to-go functions of stochastic dynamic programming by piecewise linear functions. No state discretization is necessary, and the combinatorial ``explosion'' with the number of states (the well known ``curse of dimensionality'' of dynamic programming) is avoided. The piecewise functions are obtained from the dual solutions of the optimization problem at each stage and correspond to Benders cuts in a stochastic, multistage decomposition framework. A case study of optimal stochastic scheduling for a 39-reservoir system is presented and discussed.},
issn={1436-4646},
doi={10.1007/BF01582895},
url={https://doi.org/10.1007/BF01582895}
}
%Standardwerk zu stochastischer Optimierung
@book{BirgeLouveaux,
author = {Birge, John R. and Louveaux, Francois},
title = {Introduction to Stochastic Programming},
year = {2011},
isbn = {1461402360},
publisher = {Springer Publishing Company, Incorporated},
edition = {2nd},
abstract = {The aim of stochastic programming is to find optimal decisions in problems which involve uncertain data. This field is currently developing rapidly with contributions from many disciplines including operations research, mathematics, and probability. At the same time, it is now being applied in a wide variety of subjects ranging from agriculture to financial planning and from industrial engineering to computer networks. This textbook provides a first course in stochastic programming suitable for students with a basic knowledge of linear programming, elementary analysis, and probability. The authors aim to present a broad overview of the main themes and methods of the subject. Its prime goal is to help students develop an intuition on how to model uncertainty into mathematical problems, what uncertainty changes bring to the decision process, and what techniques help to manage uncertainty in solving the problems.In this extensively updated new edition there is more material on methods and examples including several new approaches for discrete variables, new results on risk measures in modeling and Monte Carlo sampling methods, a new chapter on relationships to other methods including approximate dynamic programming, robust optimization and online methods.The book is highly illustrated with chapter summaries and many examples and exercises. Students, researchers and practitioners in operations research and the optimization area will find it particularly of interest. Review of First Edition:"The discussion on modeling issues, the large number of examples used to illustrate the material, and the breadth of the coverage make'Introduction to Stochastic Programming' an ideal textbook for the area." (Interfaces, 1998)}
}


@book{Kall_und_Wallace_Stochatic_Programming,
author = {Kall, Peter and Wallace, Stein},
year = {1994},
pages = {320},
title = {Stochastic Programming},
volume = {46},
journal = {The Journal of the Operational Research Society},
doi = {10.2307/2584504}
}

@book{Lectures_on_stochastic_Programming_Shapiro_Ruszczynski,
author = {Shapiro, Alexander and Dentcheva, Darinka and Ruszczynski, Andrzej},
title = {Lectures on Stochastic Programming: Modeling and Theory, Second Edition},
year = {2014},
isbn = {1611973422},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {Optimization problems involving stochastic models occur in almost all areas of science and engineering, such as telecommunications, medicine, and finance. Their existence compels a need for rigorous ways of formulating, analyzing, and solving such problems. This book focuses on optimization problems involving uncertain parameters and covers the theoretical foundations and recent advances in areas where stochastic models are available. In Lectures on Stochastic Programming: Modeling and Theory, Second Edition, the authors introduce new material to reflect recent developments in stochastic programming, including: an analytical description of the tangent and normal cones of chance constrained sets; analysis of optimality conditions applied to nonconvex problems; a discussion of the stochastic dual dynamic programming method; an extended discussion of law invariant coherent risk measures and their Kusuoka representations; and in-depth analysis of dynamic risk measures and concepts of time consistency, including several new results. Audience: This book is intended for researchers working on theory and applications of optimization. It also is suitable as a text for advanced graduate courses in optimization.}
}

%Rebennack Paper zu SDDP
@Article{Rebennack2016,
author={Rebennack, Steffen},
title={Combining sampling-based and scenario-based nested Benders decomposition methods: application to stochastic dual dynamic programming},
journal={Mathematical Programming},
year={2016},
volume={156},
pages={343-389},
abstract={Nested Benders decomposition is a widely used and accepted solution methodology for multi-stage stochastic linear programming problems. Motivated by large-scale applications in the context of hydro-thermal scheduling, in 1991, Pereira and Pinto introduced a sampling-based variant of the Benders decomposition method, known as stochastic dual dynamic programming (SDDP). In this paper, we embed the SDDP algorithm into the scenario tree framework, essentially combining the nested Benders decomposition method on trees with the sampling procedure of SDDP. This allows for the incorporation of different types of uncertainties in multi-stage stochastic optimization while still maintaining an efficient solution algorithm. We provide an illustration of the applicability of our method towards a least-cost hydro-thermal scheduling problem by examining an illustrative example combining both fuel cost with inflow uncertainty and by studying the Panama power system incorporating both electricity demand and inflow uncertainties.},
issn={1436-4646},
doi={10.1007/s10107-015-0884-3},
url={https://doi.org/10.1007/s10107-015-0884-3}
}
%-------------------------------------------------------------------------------------------------------------------------
%Use-Cases für Stochastic Programming

@article{article,
author = {Yen, Joyce and W, Joyce and Birge, John and R, John},
year = {2006},
pages = {3-14},
title = {A Stochastic Programming Approach to the Airline Crew Scheduling Problem},
volume = {40},
journal = {Transportation Science},
doi = {10.1287/trsc.1050.0138}
}

%-------------------------------------------------------------------------------------------------------------------------
%Theorie zu ADP
@book{Powell_solving_Curses_of_Dimensionality,
title = {Approximate dynamic programming : Solving the curses of dimensionality},
series = {Wiley series in probability and statistics},
author = {Powell, Warren B.},
address = {Hoboken, N.J},
publisher = {Wiley},
year = {2011},
edition = {2nd ed (Online-Ausg.)},
isbn = {9781283273701; 1283273705; 9781118029152},
size = {Online-Ressource (1 online resource (xviii, 627 p.)) : ill.},
keywords = {Dynamische Optimierung ; Dynamic programming ; Dynamische Optimierung},
note = {Includes bibliographical references and index. - Description based on print version record},
bestand = {Karlsruher Institut für Technologie (KIT) - KIT-Bibliothek <90>;
},
URL = {http://swbplus.bsz-bw.de/bsz353876828cov.htm ; http://zbmath.org/?q=an:1242.90002http://lib.myilibrary.com/detail.asp?id=327370},
}

@book{Bellman1957,
title = {Dynamic programming},
author = {Bellman, Richard},
address = {Princeton, NJ},
publisher = {Princeton University Press},
year = {1957},
size = {XXV, 341 S. : graph. Darst.},
bestand = {Universität Tübingen, Fachbibliothek Mathematik und Physik <21/39> [Signatur: Bell];
Universität Tübingen, Fachbibliothek Wirtschaftswissenschaft <21/19> [Signatur: Sq 40];
Universität Tübingen, Fachbibliothek Mathematik und Physik <21/98> [Signatur: Bell];
Universität Freiburg, Mathematisches Institut, <25/3e> [Signatur: Frei 3e: V/Bellman, Richard 1];
Universität Freiburg, Volkswirtschaftliches Seminar <25/10> [Signatur: Frei 10: S12/551];
Universität Freiburg, Mathematisches Institut, <25/3a> [Signatur: Frei 3a: V/Bellman];
Universität des Saarlandes, Wirtschaftswiss. Seminarbibliothek, <291/114> [Signatur: BK-3-53 / SB];
Universität Konstanz, Kommunikations-, Informations-, Medienzentrum (KIM) <352> [Signatur: mat 9:be55/d96];
Universität Mannheim, Universitätsbibliothek <180> [Signatur: DP/BEL];
Universitätsbibliothek Leipzig <15>;
Universität Heidelberg, Campus-Bibliothek Bergheim <16/160> [Signatur: WS/SK 880 B445];
Universität Heidelberg <16/7> [Signatur: Bellm];
Karlsruher Institut für Technologie (KIT) - KIT-Bibliothek <90> [Signatur: 69 A 1906];
KIT, Campus Süd, Mathematische Bibliothek <90/5>;
KIT, Campus Süd, Institut für Mess- und Regelungstechnik <90/99>;
KIT, Campus Süd, Institut für Industrielle Informationstechnik <90/159>;
Universitätsbibliothek Stuttgart <93> [Signatur: 2H 1743];
Universität Stuttgart, Fakultätsbibliothek Luft- und <93/163> [Signatur: 8.9/307];
Technische Universität Chemnitz <Ch 1>;
Leibniz-Institut für Deutsche Sprache (IDS), Bibliothek <Mh 39> [Signatur: ME 32];
},
}


%guter Überblick über ADP und Probleme
%für gute Abwechslung bei Zitieren zwischen Powell 2011 und Powell 2016

@inbook{Powell_Clearing_the_Jungle_of_stochastic_Optimization,
author = {Warren B. Powell},
title = {Clearing the Jungle of Stochastic Optimization},
booktitle = {Bridging Data and Decisions},
chapter = {Chapter 4},
pages = {109-137},
year={2014},
doi = {10.1287/educ.2014.0128},
URL = {https://pubsonline.informs.org/doi/abs/10.1287/educ.2014.0128},
eprint = {https://pubsonline.informs.org/doi/pdf/10.1287/educ.2014.0128},
    abstract = { Whereas deterministic optimization enjoys an almost universally accepted canonical form, stochastic optimization is a jungle of competing notational systems and algorithmic strategies. This is especially problematic in the context of sequential (multistage) stochastic optimization problems, which is the focus of our presentation. In this article, we place a variety of competing strategies into a common framework, which makes it easier to see the close relationship between communities such as stochastic programming, (approximate) dynamic programming, simulation, and stochastic search. What have previously been viewed as competing approaches (e.g., simulation versus optimization, stochastic programming versus dynamic programming) can be reduced to four fundamental classes of policies that are evaluated in a simulation-based setting we call the base model. The result is a single coherent framework that encompasses all of these methods, which can often be combined to create powerful hybrid policies to address complex problems. }
}


@Article{Powell_Perspectives_of_ADP,
author={Powell, Warren B.},
title={Perspectives of approximate dynamic programming},
journal={Annals of Operations Research},
year={2016},
volume={241},
number={1},
pages={319-356},
abstract={Approximate dynamic programming has evolved, initially independently, within operations research, computer science and the engineering controls community, all searching for practical tools for solving sequential stochastic optimization problems. More so than other communities, operations research continued to develop the theory behind the basic model introduced by Bellman with discrete states and actions, even while authors as early as Bellman himself recognized its limits due to the ``curse of dimensionality'' inherent in discrete state spaces. In response to these limitations, subcommunities in computer science, control theory and operations research have developed a variety of methods for solving different classes of stochastic, dynamic optimization problems, creating the appearance of a jungle of competing approaches. In this article, we show that there is actually a common theme to these strategies, and underpinning the entire field remains the fundamental algorithmic strategies of value and policy iteration that were first introduced in the 1950's and 60's.},
issn={1572-9338},
doi={10.1007/s10479-012-1077-6},
url={https://doi.org/10.1007/s10479-012-1077-6}
}

@Article{George_Powell_Stepsizes_Review,
author={George, Abraham P.
and Powell, Warren B.},
title={Adaptive stepsizes for recursive estimation with applications in approximate dynamic programming},
journal={Machine Learning},
year={2006},
volume={65},
number={1},
pages={167-198},
abstract={We address the problem of determining optimal stepsizes for estimating parameters in the context of approximate dynamic programming. The sufficient conditions for convergence of the stepsize rules have been known for 50 years, but practical computational work tends to use formulas with parameters that have to be tuned for specific applications. The problem is that in most applications in dynamic programming, observations for estimating a value function typically come from a data series that can be initially highly transient. The degree of transience affects the choice of stepsize parameters that produce the fastest convergence. In addition, the degree of initial transience can vary widely among the value function parameters for the same dynamic program. This paper reviews the literature on deterministic and stochastic stepsize rules, and derives formulas for optimal stepsizes for minimizing estimation error. This formula assumes certain parameters are known, and an approximation is proposed for the case where the parameters are unknown. Experimental work shows that the approximation provides faster convergence than other popular formulas.},
issn={1573-0565},
doi={10.1007/s10994-006-8365-9},
url={https://doi.org/10.1007/s10994-006-8365-9}
}



@book{BertsekasVol1,
title = {Dynamic programming and optimal control},
volume = {1},
author = {Bertsekas, Dimitri P.},
address = {Belmont, Mass.},
publisher = {Athena Scientific},
year = {2017},
edition = {Fourth edition},
isbn = {1886529434},
size = {XIX, 555 Seiten : Illustrationen},
keywords = { ; Dynamische Optimierung / Optimale Kontrolle},
bestand = { Universität Freiburg, Bibliothek der Technischen Fakultät <25/91> [Signatur: Frei 91: HB/A.4/67a-1];
Universität Mannheim, Universitätsbibliothek <18> [Signatur: 373 QH 423 B551-1(4)];
Karlsruher Institut für Technologie (KIT) - KIT-Bibliothek <90> [Signatur: 96 A 2567-1(4)];
Max-Planck-Institute Stuttgart, Bibliothek <Stg 113> [Signatur: SK 880 B551(4)-1];
},
}

@book{BertsekasVol2,
title = {Dynamic programming and optimal control},
volume = {2: Approximate dynamic programming},
author = {Bertsekas, Dimitri P.},
address = {Belmont, Mass.},
publisher = {Athena Scientific},
year = {2012},
edition = {4. ed.},
isbn = {9781886529441},
size = {XVII, 694 S. : graph. Darst.},
keywords = { ; Dynamische Optimierung / Optimale Kontrolle},
bestand = {Universität Tübingen, Fachbibliothek Mathematik und Physik <21/39> [Signatur: Bert];
Universität Freiburg, Bibliothek der Technischen Fakultät <25/91> [Signatur: Frei 91: SB/D.1/51-2];
Sächsische Landesbibliothek - Staats- und Universitätsbibliothek <14>;
Universität Konstanz, Kommunikations-, Informations-, Medienzentrum (KIM) <352>;
Universität Mannheim, Universitätsbibliothek <180> [Signatur: 373 QH 423 B551-2(4)];
Karlsruher Institut für Technologie (KIT) - KIT-Bibliothek <90> [Signatur: 96 A 2567-2(4)];
Technische Universität Chemnitz <Ch 1>;
Max-Planck-Institute Stuttgart, Bibliothek <Stg 113> [Signatur: SK 880 B551(4)-2];
Max Planck Institute for Software Systems <Sa 18a>;
Max-Planck-Haus (MPI) Tübingen <B 107> [Signatur: G 311b];
},
}


@article{vanRoy_Einführung_von_PostDecisionState,
title = "Neuro-dynamic programming approach to retailer inventory management",
abstract = "We discuss an application of neuro-dynamic programming techniques to the optimization of retailer inventory systems. We describe a specific case study involving a model with thirty-three state variables. The enormity of this state space renders classical algorithms of dynamic programming inapplicable. We compare the performance of solutions generated by neuro-dynamic programming algorithms to that delivered by optimized s-type (`order-up-to') policies. We are able to generate control strategies substantially superior, reducing inventory costs by approximately ten percent.",
author = "{Van Roy}, Benjamin and Bertsekas, {Dimitri P.} and Yuchun Lee and Tsitsiklis, {John N.}",
year = "1997",
language = "English (US)",
volume = "4",
pages = "4052--4057",
journal = "Proceedings of the IEEE Conference on Decision and Control",
issn = "0191-2216",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
note = "Proceedings of the 1997 36th IEEE Conference on Decision and Control. Part 1 (of 5) ; Conference date: 10-12-1997 Through 12-12-1997",
}




%Einführung für Möglichkeiten von parametrischen Methoden zu VFA
@book{Neuro-dynamic-programming,
title = {Neuro-dynamic programming},
series = {Optimization and neural computation series ; 3},
author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},
address = {Belmont, Mass.},
publisher = {Athena Scientific},
year = {1996},
isbn = {1886529108},
size = {XIII, 491 S. : graph. Darst.},
keywords = { ; Dynamic programming / Neural networks (Computer science) ; Dynamische Optimierung / Neuronales Netz / Programmierung ; Stochastische dynamische Optimierung / Neuronales Netz},
bestand = { Universität Freiburg, Bibliothek der Technischen Fakultät <25/91> [Signatur: Frei 91: SB/D.2/5];
Sächsische Landesbibliothek - Staats- und Universitätsbibliothek <14> [Signatur: 1998 8 012006];
Universitätsbibliothek Leipzig <15>;
Universität Heidelberg <16/7> [Signatur: Berts];
Karlsruher Institut für Technologie (KIT) - KIT-Bibliothek <90> [Signatur: 98 A 3564];
KIT, Campus Süd, Mathematische Bibliothek <90/5>;
KIT, Campus Süd, Bibliothek der Fakultät für Informatik <90/148> [Signatur: F.Ber(22026)];
Universitätsbibliothek Stuttgart <93>;
Technische Universität Chemnitz <Ch 1> [Signatur: SK 880 ber];
Kommunikations- u. Informationszentrum Universität Ulm (kiz) <289> [Signatur: QAA 155/1996 B];
TU Bergakademie Freiberg, Universitätsbibliothek <105> [Signatur: 03.740 8.];
Pädagogische Hochschule Weingarten, Hochschulbibliothek <747> [Signatur: SK 880 B551 N4];
Zentrum für Europäische Wirtschaftsforschung GmbH, Bibliothek <Mh 36> [Signatur: C 6/0038];
Max-Planck-Institut für Informatik <Sa 18>;
},
}

@article{Simao-Powell-Schneider-National,
author = {Simão, Hugo P. and Day, Jeff and George, Abraham P. and Gifford, Ted and Nienow, John and Powell, Warren B.},
title = {An Approximate Dynamic Programming Algorithm for Large-Scale Fleet Management: A Case Application},
journal = {Transportation Science},
volume = {43},
number = {2},
pages = {178-197},
year = {2009},
doi = {10.1287/trsc.1080.0238},

URL = { 
        https://doi.org/10.1287/trsc.1080.0238
    
},
eprint = { 
        https://doi.org/10.1287/trsc.1080.0238
    
}
,
    abstract = { We addressed the problem of developing a model to simulate at a high level of detail the movements of over 6,000 drivers for Schneider National, the largest truckload motor carrier in the United States. The goal of the model was not to obtain a better solution but rather to closely match a number of operational statistics. In addition to the need to capture a wide range of operational issues, the model had to match the performance of a highly skilled group of dispatchers while also returning the marginal value of drivers domiciled at different locations. These requirements dictated that it was not enough to optimize at each point in time (something that could be easily handled by a simulation model) but also over time. The project required bringing together years of research in approximate dynamic programming, merging math programming with machine learning, to solve dynamic programs with extremely high-dimensional state variables. The result was a model that closely calibrated against real-world operations and produced accurate estimates of the marginal value of 300 different types of drivers. }
}

@PhdThesis{Watkins:1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  url = "http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf",
  bib2html_rescat = "Parameter",
}

@book{SuttonBartoRL,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}


@article{SalasPowell-Benchmarking,
author = {Salas, Daniel and Powell, Warren},
year = {2018},
pages = {106-123},
title = {Benchmarking a Scalable Approximate Dynamic Programming Algorithm for Stochastic Control of Grid-Level Energy Storage},
volume = {30},
journal = {INFORMS Journal on Computing},
doi = {10.1287/ijoc.2017.0768}
}



%-------------------------------------------------------------------------------------------------------------------------
%Vielversprechender Ansatz und auch Use-Cases
@article{CHEN_CERVELLERA_WATER_reservoir2006,
title = "Optimization of a large-scale water reservoir network by stochastic dynamic programming with efficient state space discretization",
journal = "European Journal of Operational Research",
volume = "171",
number = "3",
pages = "1139 - 1151",
year = "2006",
note = "Feature Cluster: Heuristic and Stochastic Methods in Optimization Feature Cluster: New Opportunities for Operations Research",
issn = "0377-2217",
doi = "https://doi.org/10.1016/j.ejor.2005.01.022",
url = "http://www.sciencedirect.com/science/article/pii/S0377221705001311",
author = "Cristiano Cervellera and Victoria C.P. Chen and Aihong Wen",
keywords = "Dynamic programming, Large-scale optimization, Applied probability, Neural networks, Natural resources",
abstract = "A numerical solution to a 30-dimensional water reservoir network optimization problem, based on stochastic dynamic programming, is presented. In such problems the amount of water to be released from each reservoir is chosen to minimize a nonlinear cost (or maximize benefit) function while satisfying proper constraints. Experimental results show how dimensionality issues, given by the large number of basins and realistic modeling of the stochastic inflows, can be mitigated by employing neural approximators for the value functions, and efficient discretizations of the state space, such as orthogonal arrays, Latin hypercube designs and low-discrepancy sequences."
}
@article{Boxin_Tang_OA_Theorie,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2291282},
 abstract = {In this article, we use orthogonal arrays (OA's) to construct Latin hypercubes. Besides preserving the univariate stratification properties of Latin hypercubes, these strength r OA-based Latin hypercubes also stratify each r-dimensional margin. Therefore, such OA-based Latin hypercubes provide more suitable designs for computer experiments and numerical integration than do general Latin hypercubes. We prove that when used for integration, the sampling scheme with OA-based Latin hypercubes offers a substantial improvement over Latin hypercube sampling.},
 author = {Boxin Tang},
 journal = {Journal of the American Statistical Association},
 number = {424},
 pages = {1392--1397},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Orthogonal Array-Based Latin Hypercubes},
 volume = {88},
 year = {1993}
}



@article{CHEN_OA/MARS-Theorie,
 ISSN = {0030364X, 15265463},
 URL = {http://www.jstor.org/stable/222892},
 abstract = {In stochastic dynamic programming (SDP) with continuous state and decision variables, the future value function is computed at discrete points in the state space. Interpolation can be used to approximate the values of the future value function between these discrete points. However, for large dimensional problems the number of discrete points required to obtain a good approximation of the future value function can be prohibitively large. Statistical methods of experimental design and function estimation may be employed to overcome this "curse of dimensionality". In this paper, we describe a method for estimating the future value function by multivariate adaptive regression splines (MARS) fit over a discretization scheme based on orthogonal array (OA) experimental designs. Because orthogonal arrays only grow polynomially in the state-space dimension, our OA/MARS method is accurately able to solve higher dimensional SDP problems than previously possible. To our knowledge, the most efficient method published prior to this work employs tensor-product cubic splines to approximate the future value function (Johnson et al. 1993). The computational advantages of OA/MARS are demonstrated in comparisons with the method using tensor-product cubic splines for applications of an inventory forecasting SDP with up to nine state variables computed on a small workstation. In particular, the storage of an adequate tensor-product cubic spline for six dimensions exceeds the memory of our workstation, and the run time for an accurate OA/MARS SDP solution would be at least an order of magnitude faster than using tensor-product cubic splines for higher than six dimensions.},
 author = {Victoria C. P. Chen and David Ruppert and Christine A. Shoemaker},
 journal = {Operations Research},
 number = {1},
 pages = {38--53},
 publisher = {INFORMS},
 title = {Applying Experimental Design and Regression Splines to High-Dimensional Continuous-State Stochastic Dynamic Programming},
 volume = {47},
 year = {1999}
}

@article{Chen_orthogonal__Arrays,
title = {Application of orthogonal arrays and MARS to inventory forecasting stochastic dynamic programs},
journal = {Computational Statistics & Data Analysis},
volume = {30},
number = {3},
pages = {317-341},
year = {1999},
issn = {0167-9473},
doi = {https://doi.org/10.1016/S0167-9473(98)00084-X},
url = {https://www.sciencedirect.com/science/article/pii/S016794739800084X},
author = {Victoria C.P Chen},
keywords = {Dynamic programming, Orthogonal arrays, Regression splines},
abstract = {This paper describes the solution to inventory forecasting problems using a statistical perspective of the high-dimensional continuous-state stochastic dynamic programming (SDP) optimization model. In particular, the accuracy of the OA/MARS SDP solution method (Chen et al., 1999, Oper. Res., to appear), which employs orthogonal arrays and multivariate adaptive regression splines, is examined via simulations which vary certain user-specified parameters. For continuous-state SDP, the current definition of high-dimensional is more than five state variables. Most continuous-state problems require an approximate solution through discretization of the state space and estimation of the future value function. Under a statistical perspective, the discretization and the future value function are analogous to an experimental design and an unknown mean response.}
}
@article{CERVELLERA_Chen_NeuralNetworks,
title = {Neural network and regression spline value function approximations for stochastic dynamic programming},
journal = {Computers & Operations Research},
volume = {34},
number = {1},
pages = {70-90},
year = {2007},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2005.02.043},
url = {https://www.sciencedirect.com/science/article/pii/S030505480500136X},
author = {Cristiano Cervellera and Aihong Wen and Victoria C.P. Chen},
keywords = {Design of experiments, Statistical modeling, Markov decision process, Orthogonal array, Latin hypercube, Inventory forecasting, Water reservoir management},
abstract = {Dynamic programming is a multi-stage optimization method that is applicable to many problems in engineering. A statistical perspective of value function approximation in high-dimensional, continuous-state stochastic dynamic programming (SDP) was first presented using orthogonal array (OA) experimental designs and multivariate adaptive regression splines (MARS). Given the popularity of artificial neural networks (ANNs) for high-dimensional modeling in engineering, this paper presents an implementation of ANNs as an alternative to MARS. Comparisons consider the differences in methodological objectives, computational complexity, model accuracy, and numerical SDP solutions. Two applications are presented: a nine-dimensional inventory forecasting problem and an eight-dimensional water reservoir problem. Both OAs and OA-based Latin hypercube experimental designs are explored, and OA space-filling quality is considered.}
}

%-------------------------------------------------------------------------------------------------------------------------
%Use Cases

@article{Simao_Powell-Schneider-National,
author = {Simão, Hugo P. and Day, Jeff and George, Abraham P. and Gifford, Ted and Nienow, John and Powell, Warren B.},
title = {An Approximate Dynamic Programming Algorithm for Large-Scale Fleet Management: A Case Application},
journal = {Transportation Science},
volume = {43},
number = {2},
pages = {178-197},
year = {2009},
doi = {10.1287/trsc.1080.0238},
URL = {https://doi.org/10.1287/trsc.1080.0238},
eprint = {https://doi.org/10.1287/trsc.1080.0238}
,
    abstract = { We addressed the problem of developing a model to simulate at a high level of detail the movements of over 6,000 drivers for Schneider National, the largest truckload motor carrier in the United States. The goal of the model was not to obtain a better solution but rather to closely match a number of operational statistics. In addition to the need to capture a wide range of operational issues, the model had to match the performance of a highly skilled group of dispatchers while also returning the marginal value of drivers domiciled at different locations. These requirements dictated that it was not enough to optimize at each point in time (something that could be easily handled by a simulation model) but also over time. The project required bringing together years of research in approximate dynamic programming, merging math programming with machine learning, to solve dynamic programs with extremely high-dimensional state variables. The result was a model that closely calibrated against real-world operations and produced accurate estimates of the marginal value of 300 different types of drivers. }
}
%----------------------------------------------------------------------------------------------------------------------
%Sehr gut zum Schreiben (Verweise)
@article{article,
author = {Kleywegt, Anton and Nori, Vijay and Savelsbergh, Martin},
year = {2004},
pages = {42-70},
title = {Dynamic Programming Approximations for a Stochastic Inventory Routing Problem},
volume = {38},
journal = {Transportation Science},
doi = {10.1287/trsc.1030.0041}
}
%-----------------------------------------------------------------------------------------------------------------

%TUM Paper
@inproceedings{ record00038,
	author = {Schütz, H.-J. and Kolisch, R.},
	title = {Approximate dynamic programming for capacity allocation in the service industry},
	booktitle = {LANCS Initiative Seminar Series},
	year = {2009},
	url = {;},
}


@article{article,
author = {Das, Tapas and Gosavi, Abhijit and Mahadevan, Sridhar and Marchalleck, Nicholas},
year = {1998},
pages = {},
title = {Solving Semi-Markov Decision Problems Using Average Reward Reinforcement Learning},
volume = {45},
journal = {Management Science},
doi = {10.1287/mnsc.45.4.560}
}

@article{Use-of-Approximate-Dynamic-Programming-for-Production-Optimization,
author = {Wen, Zheng and Durlofsky, Louis and Roy, Benjamin and Aziz, Khalid},
year = {2011},
pages = {},
title = {Use of Approximate Dynamic Programming for Production Optimization},
volume = {1},
journal = {Society of Petroleum Engineers - SPE Reservoir Simulation Symposium 2011},
doi = {10.2118/141677-MS}
}

@inproceedings{NIPS2011_4311359e,
 author = {Kroemer, Oliver and Peters, Jan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Shawe-Taylor and R. Zemel and P. Bartlett and F. Pereira and K. Q. Weinberger},
 pages = {1719--1727},
 publisher = {Curran Associates, Inc.},
 title = {A Non-Parametric Approach to Dynamic Programming},
 url = {https://proceedings.neurips.cc/paper/2011/file/4311359ed4969e8401880e3c1836fbe1-Paper.pdf},
 volume = {24},
 year = {2011}
}

@article{Wen-and-Durlofsky-and-Roy-and-AzizOptimizing-Oil-Production,
author = {Wen, Zheng and Durlofsky, Louis and Roy, Benjamin and Aziz, Khalid},
year = {2013},
pages = {560-581},
title = {Approximate Dynamic Programming for Optimizing Oil Production},
isbn = {9781118104200},
journal = {Reinforcement Learning and Approximate Dynamic Programming for Feedback Control},
doi = {10.1002/9781118453988.ch25}
}

@article{article,
author = {Farias, Vivek and Roy, Benjamin},
year = {2007},
pages = {},
title = {An approximate dynamic programming approach to network revenue management}
}

@InProceedings{10.1007/11664550_13,
author="Cogill, Randy
and Rotkowitz, Michael
and Van Roy, Benjamin
and Lall, Sanjay",
editor="Francis, Bruce A.
and Smith, Malcolm C.
and Willems, Jan C.",
title="An Approximate Dynamic Programming Approach to Decentralized Control of Stochastic Systems",
booktitle="Control of Uncertain Systems: Modelling, Approximation, and Design",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="243--256",
abstract="We consider the problem of computing decentralized control policies for stochastic systems with finite state and action spaces. Synthesis of optimal decentralized policies for such problems is known to be NP-hard [1]. Here we focus on methods for efficiently computing meaningful suboptimal decentralized control policies. The algorithms we present here are based on approximation of optimal Q-functions. We show that the performance loss associated with choosing decentralized policies with respect to an approximate Q-function is related to the approximation error.",
isbn="978-3-540-31755-5"
}
%-----------------------------------------------------------------------------------------------------------------------
%Irgendwas dazwischen
@inproceedings{10.5555/777092.777140,
author = {Poupart, Pascal and Boutilier, Craig and Patrascu, Relu and Schuurmans, Dale},
title = {Piecewise Linear Value Function Approximation for Factored MDPs},
year = {2002},
isbn = {0262511290},
publisher = {American Association for Artificial Intelligence},
address = {USA},
abstract = {A number of proposals have been put forth in recent years for the solution of Markov decision processes (MDPs) whose state (and sometimes action) spaces are factored. One recent class of methods involves linear value function approximation, where the optimal value function is assumed to be a linear combination of some set of basis functions, with the aim of finding suitable weights. While sophisticated techniques have been developed for finding the best approximation within this constrained space, few methods have been proposed for choosing a suitable basis set, or modifying it if solution quality is found wanting. We propose a general framework, and specific proposals, that address both of these questions. In particular, we examine weakly coupled MDPs where a number of subtasks can be viewed independently modulo resource constraints. We then describe methods for constructing a piecewise linear combination of the subtask value functions, using greedy decision tree techniques. We argue that this architecture is suitable for many types of MDPs whose combinatorics are determined largely by the existence multiple conflicting objectives.},
booktitle = {Eighteenth National Conference on Artificial Intelligence},
pages = {292–299},
numpages = {8},
location = {Edmonton, Alberta, Canada}
}



%-------------------------------------------------------------------------------------------------------------------------
%Vermutlich nicht so nützlich --------------------------------------------------------------------------------------------
%-------------------------------------------------------------------------------------------------------------------------
@article{Beyond-stochastic-dynamic-programming,
author = {Nicol, Sam and Chadès, Iadine},
title = {Beyond stochastic dynamic programming: a heuristic sampling method for optimizing conservation decisions in very large state spaces},
journal = {Methods in Ecology and Evolution},
volume = {2},
number = {2},
pages = {221-228},
keywords = {Leipoa ocellata, Markov decision processes, metapopulation, on-line sparse sampling algorithm, optimal management, stochastic dynamic programming},
doi = {https://doi.org/10.1111/j.2041-210X.2010.00069.x},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2010.00069.x},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2010.00069.x},
abstract = {Summary 1. When managing endangered species the consequences of making a poor decision can be extinction. To make a good decision, we must account for the stochastic dynamic of the population over time. To this end stochastic dynamic programming (SDP) has become the most widely used tool to calculate the optimal policy to manage a population over time and under uncertainty. 2. However, as a result of its prohibitive computational complexity, SDP has been limited to solving small dimension problems, which results in SDP models that are either oversimplified or approximated using greedy heuristics that only consider the immediate rewards of an action. 3. We present a heuristic sampling (HS) method that approximates the optimal policy for any starting state. The method is attractive for problems with large state spaces as the running time is independent of the size of the problem state space and improves with time. 4. We demonstrate that the HS method out-performs a commonly used greedy heuristic and can quickly solve a problem with 33 million states. This is roughly 3 orders of magnitude larger than the largest problems that can currently be solved with SDP methods. 5. We found that HS out-performs greedy heuristics and can give near-optimal policies in shorter timeframes than SDP. HS can solve problems with state spaces that are too large to optimize with SDP. Where the state space size precludes SDP, we argue that HS is the best technique.},
year = {2011}
}

@article{doi:10.1057/jors.1988.26,
author = {T. S. Abdul-Razaq and C. N. Potts},
title = {Dynamic Programming State-Space Relaxation for Single-Machine Scheduling},
journal = {Journal of the Operational Research Society},
volume = {39},
number = {2},
pages = {141-152},
year  = {1988},
publisher = {Taylor & Francis},
doi = {10.1057/jors.1988.26},

URL = { 
        https://doi.org/10.1057/jors.1988.26
    
},
eprint = { 
        https://doi.org/10.1057/jors.1988.26
    
}

}